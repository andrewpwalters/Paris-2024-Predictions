{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383cd4e1-4499-4443-92d8-59537757fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import exists\n",
    "import opendatasets as od\n",
    "import shutil\n",
    "import wbgapi as wb\n",
    "\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "config = {\n",
    "    'figure.figsize':(14,4),\n",
    "    'figure.constrained_layout.use':True,\n",
    "    'figure.facecolor':'w',\n",
    "    'axes.grid':True,\n",
    "    'grid.linewidth':0.5,\n",
    "    'grid.linestyle':'--',\n",
    "    'axes.spines.top':False,\n",
    "    'axes.spines.bottom':False,\n",
    "    'axes.spines.left':False,\n",
    "    'axes.spines.right':False\n",
    "}\n",
    "\n",
    "plt.rcParams.update(config)\n",
    "\n",
    "SEED = sum(list(map(ord, 'olympics_bayes')))\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01b4622-00e7-4f08-9dda-ea0c999b476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "def get_country_code(country_name):\n",
    "    # First, try to find the country in the current countries list\n",
    "    try:\n",
    "        country = pycountry.countries.lookup(country_name)\n",
    "        return country.alpha_2\n",
    "    except LookupError:\n",
    "        # If not found, try to find it in the historic countries list\n",
    "        try:\n",
    "            historic_country = pycountry.historic_countries.lookup(country_name)\n",
    "            return historic_country.alpha_2\n",
    "        except LookupError:\n",
    "            return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2408811-73e1-4d93-9f5d-673fcbba1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def fill_future_values_last_five_years(group, value ='YR'):\n",
    "    # Sort by time to ensure correct order\n",
    "    group = group.sort_index(level=1)\n",
    "    \n",
    "    # Identify the last time period with a non-NaN value\n",
    "    last_valid_index = group[value].last_valid_index()\n",
    "    \n",
    "    if last_valid_index:\n",
    "        last_valid_time = last_valid_index[1]\n",
    "        \n",
    "        # Consider the last ten years of available non-NaN data for model fitting\n",
    "        valid_data = group.loc[group.index.get_level_values(1) <= last_valid_time].dropna(subset=[value])\n",
    "        \n",
    "        # Filter to keep only the last ten years\n",
    "        last_ten_years = valid_data.tail(5)\n",
    "        \n",
    "        if len(last_ten_years) >= 2:  # At least two data points are needed to fit a linear model\n",
    "            X = last_ten_years.index.get_level_values(1).values.reshape(-1, 1)  # Time period\n",
    "            y = last_ten_years[value].values  # Observed values\n",
    "            model = LinearRegression().fit(X, y)\n",
    "            \n",
    "            # Identify future periods to predict (after the last valid time)\n",
    "            future_data = group.loc[group.index.get_level_values(1) > last_valid_time]\n",
    "            if not future_data.empty:\n",
    "                future_periods = future_data.index.get_level_values(1).values.reshape(-1, 1)\n",
    "                \n",
    "                # Predict the next value as an extension from the last known value\n",
    "                future_predictions = model.predict(future_periods)\n",
    "                \n",
    "                # Set the predicted value for future periods\n",
    "                group.loc[future_data.index, value] = future_predictions\n",
    "                \n",
    "    return group\n",
    "\n",
    "def fill_future_values_last_ten_years(group, value ='YR'):\n",
    "    # Sort by time to ensure correct order\n",
    "    group = group.sort_index(level=1)\n",
    "    \n",
    "    # Identify the last time period with a non-NaN value\n",
    "    last_valid_index = group[value].last_valid_index()\n",
    "    \n",
    "    if last_valid_index:\n",
    "        last_valid_time = last_valid_index[1]\n",
    "        \n",
    "        # Consider the last ten years of available non-NaN data for model fitting\n",
    "        valid_data = group.loc[group.index.get_level_values(1) <= last_valid_time].dropna(subset=[value])\n",
    "        \n",
    "        # Filter to keep only the last ten years\n",
    "        last_ten_years = valid_data.tail(10)\n",
    "        \n",
    "        if len(last_ten_years) >= 2:  # At least two data points are needed to fit a linear model\n",
    "            X = last_ten_years.index.get_level_values(1).values.reshape(-1, 1)  # Time period\n",
    "            y = last_ten_years[value].values  # Observed values\n",
    "            model = LinearRegression().fit(X, y)\n",
    "            \n",
    "            # Identify future periods to predict (after the last valid time)\n",
    "            future_data = group.loc[group.index.get_level_values(1) > last_valid_time]\n",
    "            if not future_data.empty:\n",
    "                future_periods = future_data.index.get_level_values(1).values.reshape(-1, 1)\n",
    "                \n",
    "                # Predict the next value as an extension from the last known value\n",
    "                future_predictions = model.predict(future_periods)\n",
    "                \n",
    "                # Set the predicted value for future periods\n",
    "                group.loc[future_data.index, value] = future_predictions\n",
    "                \n",
    "    return group\n",
    "\n",
    "# Step 2: Extract earliest non-missing observations for each ID\n",
    "def get_earliest_non_missing(df, col):\n",
    "    earliest_non_missing = df.dropna(subset=[col]).groupby('country_code', 'year').first().reset_index()\n",
    "    return earliest_non_missing[['country_code', 'year', col]]\n",
    "\n",
    "# Step 3: Backfill missing values using a linear model\n",
    "def backfill_with_linear_model(df, col):\n",
    "    backfilled_df = df.copy()\n",
    "    for id_ in backfilled_df['country_code'].unique():\n",
    "        subset = backfilled_df[backfilled_df['country_code'] == id_]\n",
    "        missing_years = subset[subset[col].isnull()]['year']\n",
    "        \n",
    "        if not missing_years.empty:\n",
    "            # Extract earliest non-missing data point for this ID\n",
    "            earliest_data = earliest_observations[earliest_observations['country_code'] == id_]\n",
    "            if not earliest_data.empty:\n",
    "                earliest_year = earliest_data['year'].values[0]\n",
    "                earliest_value = earliest_data[col].values[0]\n",
    "                \n",
    "                # Train a simple linear model on this point\n",
    "                X_train = np.array([[earliest_year]])\n",
    "                y_train = np.array([earliest_value])\n",
    "                model = LinearRegression().fit(X_train, y_train)\n",
    "                \n",
    "                # Predict values for missing years\n",
    "                for year in missing_years:\n",
    "                    X_pred = np.array([[year]])\n",
    "                    predicted_value = model.predict(X_pred)[0]\n",
    "                    backfilled_df.loc[(backfilled_df['country_code'] == id_) & (backfilled_df['year'] == year), col] = predicted_value\n",
    "    return backfilled_df\n",
    "\n",
    "def backfill_with_linear_regression(df):\n",
    "    # Iterate through each column\n",
    "    for column in df.columns:\n",
    "        # Identify rows with non-NaN values\n",
    "        non_nan_indices = df[column].dropna().index[:5]  # Take the earliest 5 non-NaN indices\n",
    "        \n",
    "        if len(non_nan_indices) < 5:\n",
    "            print(f\"Column '{column}' does not have enough data points to perform regression.\")\n",
    "            continue\n",
    "\n",
    "        # Extract the earliest five non-NaN observations\n",
    "        y = df.loc[non_nan_indices, column].values.reshape(-1, 1)  # Dependent variable\n",
    "        X = non_nan_indices.values.reshape(-1, 1)  # Independent variable (index)\n",
    "\n",
    "        # Fit the linear regression model\n",
    "        model = LinearRegression().fit(X, y)\n",
    "\n",
    "        # Identify NaN positions to predict\n",
    "        nan_indices = df.index[df[column].isna()].values.reshape(-1, 1)\n",
    "\n",
    "        if len(nan_indices) > 0:\n",
    "            # Predict missing values\n",
    "            predicted_values = model.predict(nan_indices)\n",
    "\n",
    "            # Assign the predicted values to the corresponding NaN positions\n",
    "            df.loc[df[column].isna(), column] = predicted_values\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4f1b6-0788-4cb8-b154-759a899a2690",
   "metadata": {},
   "source": [
    "# Olympic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c37ec-c789-490c-a60b-568afea7723a",
   "metadata": {},
   "source": [
    "## Host Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "231cf9e2-9438-459c-8b7d-4d33440d5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/raw/olympic_hosts.csv\")\n",
    "\n",
    "df['game_start_date'] = pd.to_datetime(df['game_start_date'])\n",
    "\n",
    "# Year is different than game year because the Tokyo 2020 games were held in 2021\n",
    "df['year'] = df['game_start_date'].dt.year\n",
    "\n",
    "# Equestrian events took place in Sweden however the games overall were in Australia\n",
    "df.replace({'game_location': \"Australia, Sweden\"}, \"Australia\", inplace=True)\n",
    "\n",
    "df['host_city'] = df['game_name'].str[:-5]\n",
    "\n",
    "df.rename(columns={\"game_slug\":\"game_id\",\"game_location\":\"host_country\"}, inplace=True)\n",
    "\n",
    "df['host_country'] = df['host_country'].replace({\"Federal Republic of Germany\":\"Germany\",\"Great Britain\":\"United Kingdom\",\n",
    "\"Republic of Korea\":\"Korea, Republic of\", \"Yugoslavia\":\"Yugoslavia, (Socialist) Federal Republic of\", \"USSR\":\"USSR, Union of Soviet Socialist Republics\"})\n",
    "df[\"host_country\"] = df[\"host_country\"].map(lambda x: get_country_code(x))\n",
    "\n",
    "game_vars = [\"game_id\", \"year\",\"game_name\",\"host_country\",\"host_city\",\"game_season\"]\n",
    "\n",
    "df_host_all = df[game_vars]\n",
    "df = pd.read_csv(\"../Data/raw/future_games.csv\")\n",
    "df_host_all = pd.concat([df_host_all, df])\n",
    "df_host_summer = df_host_all.query(\"game_season == 'Summer'\")[game_vars]\n",
    "df_host_winter = df_host_all.query(\"game_season == 'Winter'\")[game_vars]\n",
    "\n",
    "df_host_all.to_csv(\"../Data/clean/hosts_all.csv\")\n",
    "df_host_summer.to_csv(\"../Data/clean/hosts_summer.csv\")\n",
    "df_host_winter.to_csv(\"../Data/clean/hosts_winter.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828de484-2b11-47fb-8f38-53646c38617c",
   "metadata": {},
   "source": [
    "## Participants & Medal Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c237fd9e-1712-4584-8db8-dd42e6973d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/raw/olympic_results.csv\")\n",
    "\n",
    "df['country_name'] = df['country_name'].replace(\n",
    "{\"Australasia\":\"Australia\", #Joint Aussie-Kiwi team, most were Aussies\n",
    "\"Bohemia\":\"Serbia and Montenegro\",\n",
    "\"Brunei\":\"Brunei Darussalam\",\n",
    "\"Cape Verde\":\"Cabo Verde\",\n",
    "\"Chinese Taipei\":\"Taiwan\",\n",
    "\"Czechoslovakia\":\"Serbia and Montenegro\", #This is not correct mapping (obvs), however maps to CS which was re-used\n",
    "\"Democratic Republic of the Congo\":\"Congo, The Democratic Republic of the\",\n",
    "\"German Democratic Republic (Germany)\":\"German Democratic Republic\",\n",
    "\"Great Britain\":\"United Kingdom\",\n",
    "\"Hong Kong, China\":\"Hong Kong\",\n",
    "\"Ivory Coast\":\"Côte d'Ivoire\",\n",
    "\"Malaya\":\"Malaysia\", #Formed Malaysia\n",
    "\"North Borneo\":\"Malaysia\", #Formed Malaysia\n",
    "\"Korea Team\":\"South Korea\", #DPRK and ROK had a joint team for an event in 2018, most were South Koreans\n",
    "\"Newfoundland\":\"Canada\", #Prior to joining Canada\n",
    "'Olympic Athletes from Russia':\"Russian Federation\",\n",
    "\"Palestine\":\"Palestine, State of\",\n",
    "\"ROC\":\"Russian Federation\",\n",
    "\"Republic of Korea\":\"South Korea\",\n",
    "\"Rhodesia\":\"Southern Rhodesia\",\n",
    "\"Saar\":\"Germany\",\n",
    "\"South Vietnam\":\"Viet Nam\",\n",
    "\"Soviet Union\":\"USSR, Union of Soviet Socialist Republics\",\n",
    "\"Swaziland\":\"Eswatini\",\n",
    "\"The Former Yugoslav Republic of Macedonia\":\"North Macedonia\",\n",
    "\"Turkey\":\"Türkiye\",\n",
    "\"US Virgin Islands\":'Virgin Islands, U.S.',\n",
    "\"Unified Team\":\"USSR, Union of Soviet Socialist Republics\",\n",
    "\"United Arab Republic\":'Egypt', #Brief merger of Syria and Egypt, nearly all \n",
    "\"Virgin Islands, US\":'Virgin Islands, U.S.',\n",
    "\"West Indies Federation\":'Jamaica', #Brief federation of former British colonies in the Caribbean, medal winners were mostly Jamaican\n",
    "\"Yemen Arab Republic\":\"Republic of Yemen\",\n",
    "\"Yemen Democratic Republic\":\"Yemen, Democratic, People's Democratic Republic of\",\n",
    "\"Yugoslavia\":\"Yugoslavia, (Socialist) Federal Republic of\"})\n",
    "\n",
    "df = df[~df['country_name'].isin([\"MIX\"])] #Earliest Olympics allowed teams from multiple countries, dropping\n",
    "\n",
    "df.loc[(df['slug_game'] == \"barcelona-1992\") & (df['country_name'] == \"Independent Olympic Athletes\"), 'country_name'] = \"USSR, Union of Soviet Socialist Republics\"\n",
    "df.loc[(df['slug_game'] == \"sydney-2000\") & (df['country_name'] == \"Independent Olympic Athletes\"), 'country_name'] = \"Timor-Leste\"\n",
    "df.loc[(df['slug_game'] == \"london-2012\") & (df['country_name'] == \"Independent Olympic Athletes\"), 'country_name'] = \"Netherlands Antilles\"\n",
    "df.loc[(df['slug_game'] == \"sochi-2014\") & (df['country_name'] == \"Independent Olympic Athletes\"), 'country_name'] = \"India\"\n",
    "df.loc[(df['slug_game'] == \"rio-2016\") & (df['country_name'] == \"Independent Olympic Athletes\"), 'country_name'] = \"Kuwait\"\n",
    "\n",
    "df[\"country_code\"] = df[\"country_name\"].map(lambda x: get_country_code(x))\n",
    "\n",
    "df.loc[df['country_name'] == \"Refugee Olympic Athletes\", 'country_code'] = \"UN\"\n",
    "df.loc[df['country_name'] == \"Refugee Olympic Team\", 'country_code'] = \"UN\"\n",
    "df.loc[df['country_name'] == \"Kosovo\", 'country_code'] = \"XK\"\n",
    "\n",
    "df['Gold'] = 0\n",
    "df.loc[df['medal_type'] == \"GOLD\", 'Gold'] = 1\n",
    "\n",
    "df['Silver'] = 0\n",
    "df.loc[df['medal_type'] == \"SILVER\", 'Silver'] = 1\n",
    "\n",
    "df['Bronze'] = 0\n",
    "df.loc[df['medal_type'] == \"BRONZE\", 'Bronze'] = 1\n",
    "\n",
    "df['Total'] = df['Gold'] + df['Silver'] + df['Bronze']\n",
    "df['Total_w'] = 5*df['Gold'] + 2*df['Silver'] + df['Bronze']\n",
    "\n",
    "df.rename(columns={\"slug_game\":\"game_id\"}, inplace=True)\n",
    "\n",
    "df_medal_count = df.groupby([\"game_id\",\"country_code\"]).sum()[['Gold','Silver','Bronze','Total','Total_w']]\n",
    "df_medal_count.to_csv(\"../Data/clean/medal_count.csv\")\n",
    "\n",
    "df_medal_total = df_medal_count.reset_index()[[\"game_id\",'Gold','Silver','Bronze','Total','Total_w']].groupby(\"game_id\").sum()\n",
    "df_medal_total.to_csv(\"../Data/clean/medal_total.csv\")\n",
    "\n",
    "merged_df = pd.merge(df_medal_count.reset_index(), df_medal_total.reset_index(), on='game_id', how='left', suffixes=('_df1', '_df2'))\n",
    "\n",
    "merged_df[\"Gold_pct\"] = merged_df[\"Gold_df1\"] / merged_df[\"Gold_df2\"]\n",
    "merged_df[\"Silver_pct\"] = merged_df[\"Silver_df1\"] / merged_df[\"Silver_df2\"]\n",
    "merged_df[\"Bronze_pct\"] = merged_df[\"Bronze_df1\"] / merged_df[\"Bronze_df2\"]\n",
    "merged_df[\"Total_pct\"] = merged_df[\"Total_df1\"] / merged_df[\"Total_df2\"]\n",
    "merged_df[\"Total_w_pct\"] = merged_df[\"Total_w_df1\"] / merged_df[\"Total_w_df2\"]\n",
    "\n",
    "df_medal_pct = merged_df[[\"game_id\",\"country_code\",\"Gold_pct\",\"Silver_pct\",\"Bronze_pct\",\"Total_pct\",\"Total_w_pct\"]]\n",
    "\n",
    "df_medal_pct.to_csv(\"../Data/clean/medal_pct.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf9aafd-d5ed-4e51-b95b-27475ee15ff4",
   "metadata": {},
   "source": [
    "## First Medal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29a36e1-466e-4bc2-a25d-6dcfe4911bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medal_ever = df_medal_count['Total'].reset_index()\n",
    "df_medal_ever['year'] = df_medal_ever['game_id'].str[-4:]\n",
    "\n",
    "df_medal_ever = pd.merge(df_medal_ever, df_host_all[['game_id','game_season']], on=['game_id'], how='left')\n",
    "\n",
    "df_medal_ever_all = df_medal_ever\n",
    "df_medal_ever_summer = df_medal_ever.query(\"game_season == 'Summer'\")\n",
    "df_medal_ever_winter = df_medal_ever.query(\"game_season == 'Winter'\")\n",
    "\n",
    "df_medal_ever_all = df_medal_ever_all[df_medal_ever_all['Total'] > 0][[\"year\",\"country_code\"]].groupby(\"country_code\").min().reset_index().rename(columns={\"year\":\"first_medal_all\"})\n",
    "df_medal_ever_summer = df_medal_ever_summer[df_medal_ever_summer['Total'] > 0][[\"year\",\"country_code\"]].groupby(\"country_code\").min().reset_index().rename(columns={\"year\":\"first_medal_summer\"})\n",
    "df_medal_ever_winter = df_medal_ever_winter[df_medal_ever_winter['Total'] > 0][[\"year\",\"country_code\"]].groupby(\"country_code\").min().reset_index().rename(columns={\"year\":\"first_medal_winter\"})\n",
    "\n",
    "df = df_medal_count[[]].reset_index()\n",
    "df = pd.merge(df, df_medal_ever_all, on=['country_code'], how='left')\n",
    "df = pd.merge(df, df_medal_ever_summer, on=['country_code'], how='left')\n",
    "df = pd.merge(df, df_medal_ever_winter, on=['country_code'], how='left')\n",
    "df = pd.merge(df, df_host_all[['game_id','year']], on=['game_id'], how='right')\n",
    "df = df.fillna(9999)\n",
    "\n",
    "df['won_before_all'] = df['first_medal_all'].astype('int32') < df['year']\n",
    "df['won_before_summer'] = df['first_medal_summer'].astype('int32') < df['year']\n",
    "df['won_before_winter'] = df['first_medal_winter'].astype('int32') < df['year']\n",
    "\n",
    "df_won_before = df[['game_id','country_code','won_before_all','won_before_summer','won_before_winter']]\n",
    "\n",
    "df_won_before.to_csv(\"../Data/clean/won_before.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a6f7b1-6d3b-4881-ad2b-3377a5c551c7",
   "metadata": {},
   "source": [
    "## Participants and Athlete Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "73c1989d-5cea-4128-ae68-12363e3e4320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=df_medal_count.reset_index()['game_id'].unique(), columns=df_medal_count.reset_index()['country_code'].unique())\n",
    "df = df.reset_index().melt('index').rename(columns={\"variable\":\"country_code\",\"index\":\"game_id\"})\n",
    "df = pd.merge(df[['game_id','country_code']], df_medal_count.reset_index()[['game_id','country_code','Total']], on=['game_id','country_code'], how='left')\n",
    "df[\"participant\"] = df[\"Total\"].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "df = df[[\"game_id\",\"country_code\",\"participant\"]]\n",
    "df_participant = df\n",
    "\n",
    "df = pd.read_csv(\"../Data/raw/olympics_participation.csv\") #Only includes summer games after 1960\n",
    "df = df.set_index(\"country_code\").T\n",
    "\n",
    "df_pct = df.div(df.sum(axis=1), axis=0) * 100\n",
    "\n",
    "df = df.reset_index().melt(\"index\").rename(columns={\"value\":\"athletes\",\"index\":\"game_id\"})\n",
    "\n",
    "df_pct = df_pct.reset_index().melt(\"index\").rename(columns={\"value\":\"athletes_pct\",\"index\":\"game_id\"})\n",
    "\n",
    "df = pd.merge(df_participant,df, on=['game_id','country_code'], how='left')\n",
    "df_participant = pd.merge(df_participant,df_pct, on=['game_id','country_code'], how='left')\n",
    "\n",
    "df_participant.to_csv(\"../Data/clean/participant.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e87a3-8933-4d8c-88bb-8ca0975b5506",
   "metadata": {},
   "source": [
    "# Independent Vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e6243-9651-409e-83c4-fd1373d253f4",
   "metadata": {},
   "source": [
    "## Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b3a77d-bc6a-4663-a3b0-9bea1b0261b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/2779228813.py:26: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df['DD'][:31] = round(df[\"DE\"][:31] * .21)\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/2779228813.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['DD'][:31] = round(df[\"DE\"][:31] * .21)\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/2779228813.py:27: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df['DE'][:31] = round(df[\"DE\"][:31] * (1-.21))\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/2779228813.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['DE'][:31] = round(df[\"DE\"][:31] * (1-.21))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/raw/SP_POP_TOTL.csv')\n",
    "\n",
    "for _ in range(10):\n",
    "    df[df.columns[-1][:-4] + str(int(df.columns[-1][-4:])+1)] = np.NaN\n",
    "\n",
    "df[\"country_code\"] = df[\"economy\"].map(lambda x: get_country_code(x))\n",
    "df.loc[df[\"economy\"] == \"XKX\",\"country_code\"] = \"XK\" #Kosovo\n",
    "\n",
    "df = df.loc[df['country_code'] != \"Error\"].iloc[:,1:]\n",
    "df = pd.wide_to_long(df, [\"YR\"], i=\"country_code\", j=\"year\")\n",
    "\n",
    "df = np.log(df).groupby(level=0).apply(fill_future_values_last_five_years)\n",
    "\n",
    "# Resetting the index to avoid duplicate levels\n",
    "df = df.reset_index(level=0, drop=True)\n",
    "\n",
    "df = df.reset_index().pivot(index=\"year\",columns=\"country_code\",values=\"YR\")\n",
    "df = df.interpolate()\n",
    "\n",
    "df = np.e**df\n",
    "\n",
    "df['TW'] = df.CN * .017\n",
    "df['CN'] = df.CN * (1-.017)\n",
    "\n",
    "df['DD'] = 0\n",
    "df['DD'][:31] = round(df[\"DE\"][:31] * .21)\n",
    "df['DE'][:31] = round(df[\"DE\"][:31] * (1-.21))\n",
    "\n",
    "df = backfill_with_linear_regression(df)\n",
    "df['VG'] = df['SM']\n",
    "df['KP'] = df['AU']\n",
    "\n",
    "df_pop = df\n",
    "df_pop['SU'] = df_pop.AM + df_pop.AZ + df_pop.BY + df_pop.EE + df_pop.GE + df_pop.KZ + df_pop.KG + df_pop.LV + df_pop.LT + df_pop.MD + df_pop.RU + df_pop.TJ + df_pop.TM + df_pop.UA + df_pop.UZ\n",
    "df_pop['YU'] = df_pop.BA + df_pop.HR + df_pop.MK + df_pop.ME + df_pop.RS + df_pop.SI + df_pop.XK\n",
    "df_pop['CS'] = df_pop.CZ + df_pop.SK\n",
    "df_pop['AN'] = df.CW + df.SX #Netherlands Antilles\n",
    "df_pop['RH'] = df_pop.ZW\n",
    "df_pop['UN'] = df_pop.SY\n",
    "df_pop['YD'] = df_pop.YE\n",
    "df_pop['CK'] = df_pop.SM #Cook Islands have same pop as San Marino\n",
    "df_pop = df_pop.reset_index('year').melt('year').rename(columns={\"value\":\"pop\"})\n",
    "df_pop.to_csv(\"../Data/clean/pop.csv\")\n",
    "\n",
    "df_pop_pct = df.div(df.sum(axis=1), axis=0) * 100\n",
    "df_pop_pct['SU'] = df_pop_pct.AM + df_pop_pct.AZ + df_pop_pct.BY + df_pop_pct.EE + df_pop_pct.GE + df_pop_pct.KZ + df_pop_pct.KG + df_pop_pct.LV + df_pop_pct.LT + df_pop_pct.MD + df_pop_pct.RU + df_pop_pct.TJ + df_pop_pct.TM + df_pop_pct.UA + df_pop_pct.UZ\n",
    "df_pop_pct['YU'] = df_pop_pct.BA + df_pop_pct.HR + df_pop_pct.MK + df_pop_pct.ME + df_pop_pct.RS + df_pop_pct.SI + df_pop_pct.XK\n",
    "df_pop_pct['CS'] = df_pop_pct.CZ + df_pop_pct.SK\n",
    "df_pop_pct['AN'] = df_pop_pct.CW + df_pop_pct.SX #Netherlands Antilles\n",
    "df_pop_pct['UN'] = df_pop_pct.SY\n",
    "df_pop_pct['RH'] = df_pop_pct.ZW\n",
    "df_pop_pct['YD'] = df_pop_pct.YE\n",
    "df_pop_pct['CK'] = df_pop_pct.SM #Cook Islands have same pop as San Marino\n",
    "df_pop_pct = df_pop_pct.reset_index('year').melt('year').rename(columns={\"value\":\"pct_pop\"})\n",
    "df_pop_pct.to_csv(\"../Data/clean/pct_pop.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdbc5df-8381-45ea-8e02-070a730ec670",
   "metadata": {},
   "source": [
    "## GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee77704-aac8-45ff-bbc4-cc97b86a29d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'GI' does not have enough data points to perform regression.\n",
      "Column 'KP' does not have enough data points to perform regression.\n",
      "Column 'VG' does not have enough data points to perform regression.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/861054455.py:24: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df['DD'][:31] = round(df[\"DE\"][:31] * .10)\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/861054455.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['DD'][:31] = round(df[\"DE\"][:31] * .10)\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/861054455.py:25: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df['DE'][:31] = round(df[\"DE\"][:31] * (1-.10))\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/861054455.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['DE'][:31] = round(df[\"DE\"][:31] * (1-.10))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/raw/NY.GDP.MKTP.CD.csv')\n",
    "\n",
    "for _ in range(10):\n",
    "    df[df.columns[-1][:-4] + str(int(df.columns[-1][-4:])+1)] = np.NaN\n",
    "\n",
    "df[\"country_code\"] = df[\"economy\"].map(lambda x: get_country_code(x))\n",
    "df.loc[df[\"economy\"] == \"XKX\",\"country_code\"] = \"XK\" #Kosovo\n",
    "\n",
    "df = df.loc[df['country_code'] != \"Error\"].iloc[:,1:]\n",
    "df = pd.wide_to_long(df, [\"YR\"], i=\"country_code\", j=\"year\")\n",
    "\n",
    "df = np.log(df).groupby(level=0).apply(fill_future_values_last_five_years)\n",
    "\n",
    "# Resetting the index to avoid duplicate levels\n",
    "df = df.reset_index(level=0, drop=True)\n",
    "\n",
    "df = df.reset_index().pivot(index=\"year\",columns=\"country_code\",values=\"YR\")\n",
    "df = df.interpolate()\n",
    "\n",
    "df['TW'] = df.CN * .15\n",
    "df['CN'] = df.CN * (1-.15)\n",
    "\n",
    "df['DD'] = 0\n",
    "df['DD'][:31] = round(df[\"DE\"][:31] * .10)\n",
    "df['DE'][:31] = round(df[\"DE\"][:31] * (1-.10))\n",
    "\n",
    "df = np.e**df\n",
    "\n",
    "df = backfill_with_linear_regression(df)\n",
    "df['VG'] = df['SM']\n",
    "df['KP'] = df['LA']\n",
    "\n",
    "df_gdp_total = df.div(df.sum(axis=1), axis=0) * 100\n",
    "df_gdp_total['SU'] = df_gdp_total.AM + df_gdp_total.AZ + df_gdp_total.BY + df_gdp_total.EE + df_gdp_total.GE + df_gdp_total.KZ + df_gdp_total.KG + df_gdp_total.LV + df_gdp_total.LT + df_gdp_total.MD + df_gdp_total.RU + df_gdp_total.TJ + df_gdp_total.TM + df_gdp_total.UA + df_gdp_total.UZ\n",
    "df_gdp_total['YU'] = df_gdp_total.BA + df_gdp_total.HR + df_gdp_total.MK + df_gdp_total.ME + df_gdp_total.RS + df_gdp_total.SI + df_gdp_total.XK\n",
    "df_gdp_total['CS'] = df_gdp_total.CZ + df_gdp_total.SK\n",
    "df_gdp_total['AN'] = df_gdp_total.CW + df_gdp_total.SX #Netherlands Antilles\n",
    "df_gdp_total['RH'] = df_gdp_total.ZW\n",
    "df_gdp_total['UN'] = df_gdp_total.SY\n",
    "df_gdp_total['YD'] = df_gdp_total.YE\n",
    "df_gdp_total['CK'] = df_gdp_total.MH #Cook Islands have same GDP as Marshall islands\n",
    "df_gdp_total.reset_index('year').melt('year').rename(columns={\"value\":\"pct_gdp\"}).to_csv(\"../Data/clean/pct_gdp.csv\")\n",
    "\n",
    "df_gdp = df\n",
    "df_gdp['SU'] = df_gdp.AM + df_gdp.AZ + df_gdp.BY + df_gdp.EE + df_gdp.GE + df_gdp.KZ + df_gdp.KG + df_gdp.LV + df_gdp.LT + df_gdp.MD + df_gdp.RU + df_gdp.TJ + df_gdp.TM + df_gdp.UA + df_gdp.UZ\n",
    "df_gdp['YU'] = df_gdp.BA + df_gdp.HR + df_gdp.MK + df_gdp.ME + df_gdp.RS + df_gdp.SI + df_gdp.XK\n",
    "df_gdp['CS'] = df_gdp.CZ + df_gdp.SK\n",
    "df_gdp['AN'] = df_gdp.CW + df_gdp.SX #Netherlands Antilles\n",
    "df_gdp['RH'] = df_gdp.ZW\n",
    "df_gdp['UN'] = df_gdp.SY\n",
    "df_gdp['YD'] = df_gdp.YE\n",
    "df_gdp['CK'] = df_gdp.MH #Cook Islands have same GDP as Marshall islands\n",
    "\n",
    "df_gdp.reset_index('year').melt('year').rename(columns={\"value\":\"gdp\"}).to_csv(\"../Data/clean/gdp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4303e9-999a-42b7-b4d0-eebc6490b3a5",
   "metadata": {},
   "source": [
    "## GDP per Capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2001cbf4-47d1-4057-8b1b-53e5dd480bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_gdp/df_pop.pivot(index=\"year\",columns=\"country_code\",values=\"pop\")\n",
    "df.reset_index('year').melt('year').rename(columns={\"value\":\"gdp_pc\"}).to_csv(\"../Data/clean/gdp_pc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edae66d-952d-4d84-ac27-7ba55124b1dc",
   "metadata": {},
   "source": [
    "## Freedom Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b7e411c-36f6-4d24-9b51-989a05375522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/1517577216.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\"-\", np.nan, inplace=True)\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/1517577216.py:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([new_data, df], ignore_index=True)\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/1517577216.py:79: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_data2], ignore_index=True)\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/1517577216.py:101: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df['HK'][36:] = (df['CN'][36:] + df['GB'][36:])/2\n",
      "/var/folders/w8/rmt0d09s7yd6wt42l__22p3h0000gn/T/ipykernel_83633/1517577216.py:106: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = round(df.interpolate().fillna(method='bfill').fillna(method='ffill')*2)/2\n"
     ]
    }
   ],
   "source": [
    "xl = pd.ExcelFile(\"../Data/raw/FIW_1973-2024.xls\")\n",
    "\n",
    "df = xl.parse('Country Ratings, Statuses ', skiprows=1)\n",
    "df = df.rename(columns={\"Year(s) Under Review\":\"Country\"})\n",
    "df = df.iloc[1:,:]\n",
    "\n",
    "df['Country'] = df['Country'].replace(\n",
    "{\"Brunei\":\"Brunei Darussalam\",\n",
    "\"Congo (Brazzaville)\":\"Congo\",\n",
    "\"Congo (Kinshasa)\":\"Congo, The Democratic Republic of the\",\n",
    "\"Cote d'Ivoire\":\"Côte d'Ivoire\",\n",
    "\"Czechoslovakia\":\"Serbia and Montenegro\", #This is not correct mapping (obvs), however maps to CS which was re-used\n",
    "\"Germany, E. \":\"German Democratic Republic\",\n",
    "\"Germany, W. \":\"Germany\",\n",
    "\"Micronesia\":\"Micronesia, Federated States of\",\n",
    "\"Russia\":\"Russian Federation\",\n",
    "\"St. Kitts and Nevis\":\"Saint Kitts and Nevis\",\n",
    "\"St. Lucia\":\"Saint Lucia\",\n",
    "\"St. Vincent and the Grenadines\":\"Saint Vincent and the Grenadines\",\n",
    "\"The Gambia\":\"Gambia\",\n",
    "\"Turkey\":\"Türkiye\",\n",
    "\"USSR\":\"USSR, Union of Soviet Socialist Republics\",\n",
    "\"Vietnam, N.\":\"Viet-Nam, Democratic Republic of\",\n",
    "\"Vietnam, S.\":\"Viet Nam\",\n",
    "\"Yemen, N.\":\"Republic of Yemen\",\n",
    "\"Yemen, S.\":\"Yemen, Democratic, People's Democratic Republic of\",\n",
    "\"Yugoslavia\":\"Yugoslavia, (Socialist) Federal Republic of\"})\n",
    "\n",
    "df[\"country_code\"] = df[\"Country\"].map(lambda x: get_country_code(x))\n",
    "\n",
    "df.loc[df['Country'] == \"Kosovo\", 'country_code'] = \"XK\"\n",
    "\n",
    "#South Africa for has odd data, I'm using the values from the next two years\n",
    "df.loc[(df['country_code'] == \"ZA\"), 1972] = 4\n",
    "df.loc[(df['country_code'] == \"ZA\"), \"Unnamed: 2\"] = 5\n",
    "\n",
    "df = df.loc[df['country_code'] != \"Error\"].iloc[:,1:]\n",
    "\n",
    "df.replace(\"-\", np.nan, inplace=True)\n",
    "\n",
    "df = df.set_index(\"country_code\")\n",
    "\n",
    "# Get list of all column sets\n",
    "column_sets = [df.columns[i:i+3] for i in range(0, len(df.columns), 3)]\n",
    "\n",
    "# Process each set of three columns\n",
    "dfs = []\n",
    "for columns in column_sets:\n",
    "    # Extract columns for this set\n",
    "    subset_df = df.loc[:, columns]  # Using .loc to work on a view of the original dataframe\n",
    "    \n",
    "    # Calculate sum of first column and drop the second and third columns\n",
    "    new_column_name = columns[0]\n",
    "    subset_df[new_column_name] = (subset_df[columns[0]] + subset_df[columns[1]])/2\n",
    "    \n",
    "    # Drop the second and third columns\n",
    "    subset_df.drop(columns=columns[1:], inplace=True)\n",
    "    \n",
    "    dfs.append(subset_df)\n",
    "\n",
    "# Concatenate all modified dataframes into a final dataframe\n",
    "df = pd.concat(dfs, axis=1)\n",
    "\n",
    "for i in df.columns:\n",
    "    df.rename(columns={i:(\"YR\"+str(i)[-4:])}, inplace=True)\n",
    "\n",
    "df = df.groupby('country_code').mean()\n",
    "\n",
    "df = pd.wide_to_long(df.reset_index(), [\"YR\"], i=\"country_code\", j=\"year\")\n",
    "\n",
    "df = df.reset_index().pivot(index=\"year\",columns=\"country_code\",values=\"YR\")\n",
    "\n",
    "df=df.reset_index()\n",
    "new_data = pd.DataFrame(index=range(12), columns=df.columns)\n",
    "new_data2 = pd.DataFrame(index=range(10), columns=df.columns)\n",
    "new_data['year'] = range(1960, 1972)\n",
    "new_data2['year'] = range(2024, 2034)\n",
    "df = pd.concat([new_data, df], ignore_index=True)\n",
    "df = pd.concat([df, new_data2], ignore_index=True)\n",
    "df = df.set_index('year')\n",
    "\n",
    "#We are filling in missing Freedom Index data by assigning it the value of a different state with a similar political and economic system.\n",
    "df['AN'] = df['NL']\n",
    "df['AS'] = df['US']\n",
    "df['AW'] = df['NL']\n",
    "df['BM'] = df['GB']\n",
    "df['CK'] = df['GB']\n",
    "df['GU'] = df['US']\n",
    "df['KY'] = df['GB']\n",
    "df['PR'] = df['US']\n",
    "df['RH'] = df['ZW'] #Rhodesia became Zimbabwe\n",
    "df['VG'] = df['GB']\n",
    "df['VI'] = df['US']\n",
    "\n",
    "#The West Bank is under Israeli military occupation, so we are using neighboring Jordan instead.\n",
    "df['PS'] = df['JO']\n",
    "\n",
    "#Hong Kong was a colony of the UK until 1997 when it was handed off to China-Beijing.\n",
    "#After the hand-off we are averaging the two\n",
    "df['HK'] = df['GB']\n",
    "df['HK'][36:] = (df['CN'][36:] + df['GB'][36:])/2\n",
    "\n",
    "#The country of origin with the most refugees is Syria\n",
    "df['UN'] = df['SY']\n",
    "\n",
    "df = round(df.interpolate().fillna(method='bfill').fillna(method='ffill')*2)/2\n",
    "\n",
    "df_free = df.reset_index('year').melt('year').rename(columns={\"value\":\"free\"})\n",
    "df_free.to_csv(\"../Data/clean/freedom.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
